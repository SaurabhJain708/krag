aiohappyeyeballs==2.6.1
aiohttp==3.12.7
aiosignal==1.3.2
async-timeout==5.0.1
attrs==25.3.0
cbor2==5.7.0
certifi==2026.1.4
charset-normalizer==3.4.4
einops==0.8.2
filelock==3.20.3
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl#sha256=13454dd3d37cf173649bd389b84614b8072fb283f8f2fd23a65ab66caafc304b
frozenlist==1.6.0
fsspec==2026.2.0
grpclib==0.4.8
h2==4.2.0
hf-xet==1.2.0
hpack==4.1.0
huggingface_hub==0.36.2
hyperframe==6.1.0
idna==3.11
Jinja2==3.1.6
MarkupSafe==3.0.3
mpmath==1.3.0
multidict==6.4.4
networkx==3.4.2
ninja==1.13.0
numpy==2.2.6
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.9.86
nvidia-nvtx-cu12==12.1.105
packaging==26.0
pillow==12.1.0
propcache==0.3.1
protobuf==6.31.1
PyYAML==6.0.3
regex==2026.1.15
requests==2.32.5
safetensors==0.7.0
sympy==1.14.0
timm==1.0.24
tokenizers==0.19.1
torch==2.3.0+cu121
torchvision==0.18.0+cu121
tqdm==4.67.3
transformers==4.41.2
triton==2.3.0
typing_extensions==4.15.0
urllib3==2.6.3
yarl==1.20.0
